<!doctype html>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-XPW0Q1S0D9"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-XPW0Q1S0D9');
  </script>
  <meta charset="utf-8" />
  <title>Adversarially Robust Deepfake Detection</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="A compact, adversarially hardened deepfake detector built with a hybrid EfficientNet plus Vision Transformer architecture and a dual-attacker training protocol." />
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700;800;900&display=swap" rel="stylesheet">
  <style>
    :root{
      /* Dark Cyberpunk Theme matching main site */
      --bg: #0f172a;           /* Deep Slate */
      --bg-soft: #1e293b;
      --card: rgba(30, 41, 59, 0.6); /* Dark glass card */
      --ink: #f1f5f9;          /* Off-white text */
      --muted: #94a3b8;        /* Muted text */
      --border: rgba(255, 255, 255, 0.1); /* Subtle border */
      --chip: rgba(255, 255, 255, 0.05);  /* Dark chip */
      --code: #1e293b;

      /* Accents - matching the Matrix Teal */
      --primary: #2dd4bf;       /* Teal 400 */
      --primary-hover: #14b8a6;
      --ring: rgba(45, 212, 191, 0.5);
      --font-main: "Inter", system-ui, -apple-system, sans-serif;
    }

    *{box-sizing:border-box}
    html,body{height:100%}
    body{
      margin:0; background:var(--bg); color:var(--ink);
      font-family: var(--font-main);
      line-height: 1.6;
      -webkit-font-smoothing:antialiased; -moz-osx-font-smoothing:grayscale;
      /* Background pattern from main site */
      background-image:
        radial-gradient(at 0% 0%, rgba(45, 212, 191, 0.05) 0px, transparent 50%),
        radial-gradient(at 100% 100%, rgba(30, 41, 59, 0.5) 0px, transparent 50%);
      background-attachment: fixed;
    }

    .container{max-width:1100px; margin:0 auto; padding:24px}

    header{
      position:sticky; top:0; z-index:50;
      backdrop-filter:blur(16px);
      background:rgba(15, 23, 42, 0.85);
      border-bottom:1px solid var(--border);
    }
    .header-inner{display:flex; align-items:flex-start; gap:16px; padding:12px 24px; max-width:1100px; margin:0 auto; flex-wrap:wrap}
    .logo{display:flex; align-items:center; gap:10px; text-decoration:none; color:var(--ink)}
    .logo-mark{width:36px; height:36px; border-radius:10px; background:linear-gradient(135deg, var(--primary-hover), var(--primary)); box-shadow:0 6px 20px var(--ring)}
    .logo h1{font-size:18px; margin:0; font-weight: 700;}
    .header-content{display:flex; flex-direction:column; gap:12px}
    .header-actions{display:flex; flex-wrap:wrap; gap:12px; margin-left:0px}
    @media(max-width:600px){ .header-actions{margin-left:0} }

    .pill{
      padding:10px 16px; border-radius:999px; text-decoration:none; font-weight:600; display:inline-flex; align-items:center; gap:10px;
      transition:transform .05s ease, box-shadow .2s ease, background-color .2s ease, color .2s ease, border-color .2s ease;
      box-shadow:0 8px 20px var(--ring);
      border:1px solid transparent;
      font-size: 0.9rem;
    }
    .pill.primary{background:linear-gradient(135deg, var(--primary-hover), var(--primary)); color:#0f172a; text-shadow:none;}
    .pill.primary:hover{background:var(--primary); transform:translateY(-1px)}
    .pill.ghost{background:transparent; color:var(--ink); border-color:var(--border); box-shadow:none}
    .pill.ghost:hover{background:rgba(255,255,255,0.05); transform:translateY(-1px); border-color:var(--primary); color: var(--primary)}

    main{padding:24px}

    section{background:var(--card); border:1px solid var(--border); border-radius:16px; padding:22px; backdrop-filter: blur(12px);}
    .synopsis h2{margin:0 0 10px 0}
    h2{margin:0 0 12px; font-size:24px; font-weight: 700; color: var(--ink);}
    h3{margin:16px 0 8px; font-size:18px; font-weight: 600; color: #fff;}
    p{margin:10px 0; color: var(--muted);}

    .chips{display:flex; flex-wrap:wrap; gap:8px; margin-top:8px}
    .chip{background:var(--chip); border:1px solid var(--border); padding:6px 10px; border-radius:999px; font-size:13px; color:var(--muted)}

    .grid{display:grid; gap:18px}
    @media(min-width:980px){ .grid{grid-template-columns: 280px 1fr} }
    nav.toc{
      position:sticky; top:76px; align-self:start; background:var(--card); border:1px solid var(--border); border-radius:14px; padding:14px;
      backdrop-filter: blur(12px);
    }
    nav.toc h3{margin:6px 6px 10px 6px; font-size:13px; color:var(--muted); letter-spacing:.4px; text-transform:uppercase}
    nav.toc a{
      display:block; color:var(--muted); text-decoration:none; padding:8px 10px; border-radius:10px; font-size:14px; border:1px solid transparent; transition: all 0.2s;
    }
    nav.toc a:hover{background:rgba(255,255,255,0.05); border-color:var(--border); color: var(--ink);}

    .callout{padding:12px 14px; border-left:4px solid var(--primary); background:rgba(45, 212, 191, 0.1); border-radius:10px; color: var(--ink);}

    table{width:100%; border-collapse:separate; border-spacing:0; overflow:hidden; border-radius:12px; margin:10px 0; border: 1px solid var(--border);}
    thead th{background:rgba(255,255,255,0.05); color:var(--muted); font-weight:600; text-align:left; padding:10px; border-bottom:1px solid var(--border)}
    tbody td{padding:10px; border-bottom:1px solid var(--border); color: var(--muted);}
    tbody tr:last-child td { border-bottom: none; }
    tbody tr:hover td{background:rgba(255,255,255,0.05)}

    footer{color:var(--muted); text-align:center; padding:36px 0}
    a{color:var(--primary); transition: color 0.2s;}
    a:hover{color: var(--primary-hover);}
    .small{font-size:13px; color:var(--muted)}
    .badge{display:inline-flex; align-items:center; gap:8px; background:var(--chip); padding:8px 12px; border-radius:12px; border:1px solid var(--border); color: var(--muted);}
  </style>
</head>
<body>
  <header>
    <div class="header-inner">
      <div class="header-content">
        <a class="logo" href="#">
          <h1>Adversarially Robust Deepfake Detection</h1>
        </a>
        <div class="header-actions">
          <a class="pill ghost" href="https://github.com/Shabbir-Tashrifwala/robust-deepfake-detector" target="_blank" rel="noreferrer">
            View project on GitHub
          </a>
          <a class="pill ghost" href="../index.html#projects">
            Back to Project Gallery
          </a>
        </div>
      </div>
      <div style="flex:1"></div>
    </div>
  </header>

  <main class="container">
    <section class="synopsis" id="synopsis">
      <h2>Synopsis</h2>
      <p>
        Modern deepfake detection systems have a serious weakness: they can be fooled by tiny, invisible changes made to videos. This project tackles the challenge of building detectors that remain reliable even when attackers deliberately try to trick them. Our approach involves training a deepfake detector by constantly attacking it during the learning phase. This is similar to how vaccines work, by exposing the immune system to controlled threats, we build resistance. We use two different types of attacks during training: one that rapidly finds the most damaging modifications possible, and another that learns to create subtle, realistic-looking changes. Our results show that the detector performs just as well as traditional systems on normal videos, but it maintains much better accuracy when tested on attacked and compressed videos. Additionally, when we analyze what the detector pays attention to, we find that it focuses on natural facial features across the entire face, rather than zeroing in on technical glitches that attackers can easily hide. This explains why the system is more robust.
      </p>

      <!-- Metric boxes under Synopsis were removed -->
      <div class="chips" aria-label="Key tags">
        <span class="chip">EfficientNet + ViT</span>
        <span class="chip">PGD + Learned U Net attacker</span>
        <span class="chip">TV and frequency constraints</span>
        <span class="chip">FaceForensics++ train</span>
        <span class="chip">Celeb DF v2 test</span>
        <span class="chip">Grad CAM explainability</span>
      </div>
    </section>

    <div class="grid" style="margin-top:18px">
      <nav class="toc" aria-label="Table of contents">
        <h3>Contents</h3>
        <a href="#problem">1. Problem the product solves</a>
        <a href="#solution">2. Solution overview</a>
        <a href="#architecture">3. Architecture</a>
        <a href="#attackers">4. Adversarial training</a>
        <a href="#data">5. Data pipeline and setup</a>
        <a href="#evaluation">6. Evaluation protocol</a>
        <a href="#results">7. Results at a glance</a>
        <a href="#explain">8. Explainability insights</a>
        <a href="#limits">9. Limits and future work</a>
        <a href="#faq">FAQ</a>
      </nav>

      <div class="content">
        <section id="problem">
          <h2>1. Problem the product solves</h2>
          <p>
            Deepfake media is now highly realistic. Detectors trained only for accuracy on clean benchmarks can fail when an attacker adds tiny, crafted perturbations or when common compressions wash away local artifacts. This product focuses on resilience. The aim is to keep detection performance usable even when inputs are manipulated to evade the model or passed through lossy compression.
          </p>
          <div class="callout">
            The north star metric is maintaining ROC AUC and accuracy when attacks meet compression so that screening stays dependable.
          </div>
        </section>

        <section id="solution">
          <h2>2. Solution overview</h2>
          <p>
            We harden a compact hybrid detector through a game between the model and two complementary attackers. The network faces PGD noise and a learned U Net attacker that produces realistic, spatially smooth, compression resilient patterns. The U Net is leashed by total variation loss to reduce high frequency speckle and by a frequency domain loss that discourages very low frequency energy. The result is an attacker that searches mid to high frequency space and forces the detector to learn more stable cues.
          </p>
          <div class="chips">
            <span class="chip">Compact model</span>
            <span class="chip">Dual attacker training</span>
            <span class="chip">Compression aware robustness</span>
          </div>
        </section>

        <section id="architecture">
          <h2>3. Architecture</h2>
          <h3>3.1 EfficientViT detector</h3>
          <p>
            The backbone is EfficientNet B0 used as a feature extractor that outputs a 7 by 7 by 1280 map for a 224 by 224 face crop. We reshape this grid into 49 tokens, prepend a CLS token, add learnable positions, and pass the sequence through a small Transformer encoder with 4 layers and 4 heads. The CLS output goes into a light MLP head for binary classification.
          </p>
          <div class="chips">
            <span class="chip">EfficientNet B0 features</span>
            <span class="chip">ViT encoder, 4 layers</span>
            <span class="chip">MLP head</span>
          </div>
          <h3>3.2 Why hybrid</h3>
          <p>
            CNNs capture local texture tells like edge inconsistencies. Transformers capture long range relations like lighting agreement between forehead and jaw. Together they cover both local and global cues with a small parameter budget.
          </p>
        </section>

        <section id="attackers">
          <h2>4. Adversarial training</h2>
          <h3>4.1 PGD attacker</h3>
          <p>
            We use L infinity PGD with epsilon 8 over 255, step 2 over 255, and 10 iterations. Random start is included. This gives a strong baseline adversary that finds harmful pixel level changes.
          </p>
          <h3>4.2 Learned U Net attacker</h3>
          <p>
            A tiny U Net takes the clean face crop and outputs a perturbation delta bounded in L infinity by epsilon through a tanh gate and scaling. The U Net tries to maximize detector loss while obeying realism constraints.
          </p>
          <h3>4.3 Realism constraints</h3>
          <ul>
            <li><strong>Total variation loss</strong> promotes spatial smoothness, which avoids speckle like artifacts.</li>
            <li><strong>Frequency loss</strong> penalizes very low frequency energy in the perturbation spectrum, which nudges the attacker toward mid to high frequency patterns that often survive compression.</li>
          </ul>
          <h3>4.4 Training loop</h3>
          <p>
            For each batch we compute clean loss, PGD loss, update the U Net with its composite attacker loss, then update the detector on a fresh U Net perturbation. The detector minimizes the sum of clean, PGD, and U Net losses. Gradient clipping and mixed precision keep the dynamics stable.
          </p>
          <div class="callout">
            The arms race pushes the detector to stop over relying on a single brittle cue and to combine broader evidence across the face.
          </div>
        </section>

        <section id="data">
          <h2>5. Data pipeline and setup</h2>
          <p>
            We extract frames from FaceForensics plus plus c23 and Celeb DF v2, detect faces with MTCNN, then align and crop to 224 by 224. Training uses a subset of FF plus plus frames. Evaluation uses clean and attacked crops from Celeb DF v2 for cross dataset testing. Preprocessing uses resizing and ImageNet normalization. Training runs on a T4 GPU with AMP.
          </p>
          <div class="chips">
            <span class="chip">FF++ c23 train</span>
            <span class="chip">Celeb DF v2 test</span>
            <span class="chip">MTCNN face crops</span>
            <span class="chip">AMP enabled</span>
          </div>
        </section>

        <section id="evaluation">
          <h2>6. Evaluation protocol</h2>
          <p>
            We compare a baseline model trained on clean data and a robust model trained with the dual attacker loop. We test clean, JPEG quality 50, an H.264 like simulation, PGD white box, learned U Net, and the combined U Net plus compression settings. We report accuracy and ROC AUC as the primary decision metrics.
          </p>
          <div class="chips">
            <span class="chip">Clean</span>
            <span class="chip">JPEG 50</span>
            <span class="chip">H.264 like</span>
            <span class="chip">PGD</span>
            <span class="chip">Learned U Net</span>
            <span class="chip">U Net + compression</span>
          </div>
        </section>

        <section id="results">
          <h2>7. Results at a glance</h2>

          <h3>7.1 Cross domain performance on Celeb DF v2</h3>
          <table aria-label="Celeb DF v2 summary">
            <thead>
              <tr>
                <th>Scenario</th>
                <th>Accuracy (Base)</th>
                <th>Accuracy (Robust)</th>
                <th>AUC (Base)</th>
                <th>AUC (Robust)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Clean</td>
                <td>0.624</td>
                <td>0.617</td>
                <td>0.678</td>
                <td>0.676</td>
              </tr>
              <tr>
                <td>JPEG 50</td>
                <td>0.625</td>
                <td>0.641</td>
                <td>0.672</td>
                <td>0.681</td>
              </tr>
              <tr>
                <td>H.264 like</td>
                <td>0.636</td>
                <td>0.661</td>
                <td>0.693</td>
                <td>0.703</td>
              </tr>
              <tr>
                <td>PGD white box</td>
                <td>0.467</td>
                <td>0.476</td>
                <td>0.460</td>
                <td>0.483</td>
              </tr>
              <tr>
                <td>Learned U Net</td>
                <td>0.619</td>
                <td>0.607</td>
                <td>0.675</td>
                <td>0.674</td>
              </tr>
              <tr>
                <td>U Net + JPEG 50</td>
                <td>0.625</td>
                <td>0.647</td>
                <td>0.675</td>
                <td>0.685</td>
              </tr>
              <tr>
                <td>U Net + H.264 like</td>
                <td>0.644</td>
                <td>0.654</td>
                <td>0.690</td>
                <td>0.699</td>
              </tr>
            </tbody>
          </table>

          <h3>7.2 In domain performance on FF plus plus c23</h3>
          <p class="small">
            Both models are near perfect on clean FF plus plus frames. Under compression the robust model keeps accuracy competitive while sustaining ROC AUC.
          </p>
          <table aria-label="FF++ c23 summary">
            <thead>
              <tr>
                <th>Scenario</th>
                <th>Accuracy (Base)</th>
                <th>Accuracy (Robust)</th>
                <th>AUC (Base)</th>
                <th>AUC (Robust)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Clean</td>
                <td>0.999</td>
                <td>0.998</td>
                <td>~1.000</td>
                <td>~1.000</td>
              </tr>
              <tr>
                <td>JPEG 50</td>
                <td>0.851</td>
                <td>0.836</td>
                <td>0.922</td>
                <td>0.916</td>
              </tr>
              <tr>
                <td>H.264 like</td>
                <td>0.912</td>
                <td>0.912</td>
                <td>0.965</td>
                <td>0.962</td>
              </tr>
            </tbody>
          </table>
        </section>

        <section id="explain">
          <h2>8. Explainability insights</h2>
          <p>
            Grad CAM comparisons show that the baseline often fires on sharp borders like jawlines or the face boundary. The robust model spreads attention over cheeks and forehead, with less spill to background. This matches the idea that adversarial training discourages single cue dependence and pushes attention toward cues that are harder to scrub out with simple edits or compression.
          </p>
          <p>
            Thinking in frequency space helps. Baseline attention aligns with very high frequency details. The robust model raises sensitivity to mid frequency patterns and smooth shading consistency, which are more durable under compression and against small edits.
          </p>
        </section>

        <section id="limits">
          <h2>9. Limits and future work</h2>
          <ul>
            <li>Strong PGD still hurts both models on faces at this resolution. Larger models or stronger training schedules could help.</li>
            <li>Cross dataset AUC around the high 0.6s shows that generalization to harder fakes remains challenging.</li>
            <li>Temporal attacks are not modeled yet. Extending realism constraints to video time could improve resilience.</li>
          </ul>
        </section>

        <section id="faq">
          <h2>FAQ</h2>
          <h3>Why a tiny U Net attacker rather than only PGD</h3>
          <p>
            PGD is strong but often looks like fine noise. A learned U Net generates patterns that are spatially coherent and better mimic small cosmetic tweaks attackers might apply. This combination reduces overfitting to one attack style.
          </p>
          <h3>Why total variation and frequency constraints</h3>
          <p>
            TV lowers speckle, which makes perturbations more plausible. Penalizing very low frequency energy avoids broad washes that compression would flatten anyway. Together they push the attacker into the band where compression is less destructive and the detector must learn more durable cues.
          </p>
          <h3>Does the robust model hurt clean accuracy</h3>
          <p>
            On clean FF plus plus and clean Celeb DF v2, robust accuracy and AUC match the baseline within noise. The advantage shows up under attack and compression, which is where it matters operationally.
          </p>
        </section>
      </div>
    </div>


  </main>
</body>
</html>
