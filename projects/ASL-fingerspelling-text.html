<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Transformer-Based ASL Fingerspelling to Text</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="Real-time ASL fingerspelling to text using MediaPipe landmarks + a Transformer. Includes a simple webcam app" />
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700;800;900&display=swap" rel="stylesheet">
  <style>
    :root{
      /* Dark Cyberpunk Theme matching main site */
      --bg: #0f172a;           /* Deep Slate */
      --bg-soft: #1e293b;
      --card: rgba(30, 41, 59, 0.6); /* Dark glass card */
      --ink: #f1f5f9;          /* Off-white text */
      --muted: #94a3b8;        /* Muted text */
      --line: rgba(255, 255, 255, 0.1); /* Subtle border */
      --chip: rgba(255, 255, 255, 0.05);  /* Dark chip */

      /* Accents - matching the Matrix Teal */
      --accent: #2dd4bf;       /* Teal 400 */
      --accent-hover: #14b8a6;
      --glow: rgba(45, 212, 191, 0.5);
      --font-main: "Inter", system-ui, -apple-system, sans-serif;
    }

    *{box-sizing:border-box}
    html,body{height:100%}
    body{
      margin:0; background:var(--bg); color:var(--ink);
      font-family: var(--font-main);
      line-height: 1.6;
      -webkit-font-smoothing:antialiased; -moz-osx-font-smoothing:grayscale;
      /* Background pattern from main site */
      background-image:
        radial-gradient(at 0% 0%, rgba(45, 212, 191, 0.05) 0px, transparent 50%),
        radial-gradient(at 100% 100%, rgba(30, 41, 59, 0.5) 0px, transparent 50%);
      background-attachment: fixed;
    }
    a{color:var(--accent); text-decoration:none; transition: color 0.2s;}
    a:hover{color: var(--accent-hover); opacity:1}

    .container{max-width:1100px; margin:0 auto; padding:24px}

    header{
      position:sticky; top:0; z-index:50;
      backdrop-filter:blur(16px);
      background:rgba(15, 23, 42, 0.85);
      border-bottom:1px solid var(--line);
    }
    .header-inner{display:flex; align-items:flex-start; gap:16px; padding:14px 24px; max-width:1100px; margin:0 auto; flex-wrap:wrap}
    .brand{display:flex; flex-direction:column; gap:8px}
    .brand h1{margin:0; font-size:20px; font-weight: 700;}
    .actions{display:flex; gap:12px; flex-wrap:wrap}

    .pill{
      display:inline-flex; align-items:center; gap:10px;
      padding:10px 16px; border-radius:999px; font-weight:600; border:1px solid transparent;
      transition:transform .06s ease, background-color .2s ease, border-color .2s ease, color .2s ease, box-shadow .2s ease;
      cursor:pointer;
      font-size: 0.9rem;
    }
    .pill.primary{background:linear-gradient(135deg, var(--accent-hover), var(--accent)); color:#0f172a; box-shadow:0 10px 24px var(--glow); text-shadow:none;}
    .pill.primary:hover{background:var(--accent); transform:translateY(-1px)}
    .pill.ghost{background:transparent; color:var(--ink); border:1px solid var(--line)}
    .pill.ghost:hover{transform:translateY(-1px); background:rgba(255,255,255,0.05); border-color:var(--accent); color:var(--accent)}

    main{padding:24px}
    section{background:var(--card); border:1px solid var(--line); border-radius:16px; padding:22px; backdrop-filter: blur(12px);}
    .synopsis h2{margin:0 0 10px}
    h2{margin:0 0 12px; font-size:24px; font-weight: 700; color: var(--ink);}
    h3{margin:16px 0 8px; font-size:18px; font-weight: 600; color: #fff;}
    p{margin:10px 0; color: var(--muted);}

    .grid{display:grid; gap:18px}
    @media(min-width:980px){ .grid{grid-template-columns:280px 1fr} }

    nav.toc{
      position:sticky; top:96px; align-self:start; background:var(--card); border:1px solid var(--line); border-radius:14px; padding:14px;
      backdrop-filter: blur(12px);
    }
    nav.toc h3{margin:6px 6px 10px; font-size:13px; color:var(--muted); letter-spacing:.4px; text-transform:uppercase}
    nav.toc a{
      display:block; color:var(--muted); padding:8px 10px; border-radius:10px; font-size:14px; border:1px solid transparent; transition: all 0.2s;
    }
    nav.toc a:hover{background:rgba(255,255,255,0.05); border-color:var(--line); color: var(--ink);}

    .chips{display:flex; flex-wrap:wrap; gap:8px; margin-top:8px}
    .chip{background:var(--chip); border:1px solid var(--line); padding:6px 10px; border-radius:999px; font-size:13px; color:var(--muted)}

    .callout{padding:12px 14px; border-left:4px solid var(--accent); background:rgba(45, 212, 191, 0.1); border-radius:10px; color: var(--ink);}

    table{width:100%; border-collapse:separate; border-spacing:0; overflow:hidden; border-radius:12px; margin:12px 0; border: 1px solid var(--line);}
    thead th{background:rgba(255,255,255,0.05); color:var(--muted); font-weight:600; text-align:left; padding:10px; border-bottom:1px solid var(--line)}
    tbody td{padding:10px; border-bottom:1px solid var(--line); color: var(--muted);}
    tbody tr:last-child td { border-bottom: none; }
    tbody tr:hover td{background:rgba(255,255,255,0.05)}

    footer{color:var(--muted); text-align:center; padding:36px 0}
    .small{font-size:13px; color:var(--muted)}
  </style>
</head>
<body>
  <header>
    <div class="header-inner">
      <div class="brand">
        <h1>Transformer-Based ASL Fingerspelling to Text</h1>
        <div class="actions">
          <a class="pill ghost" href="https://github.com/Shabbir-Tashrifwala/ASL-Fingerspelling-to-Text" target="_blank" rel="noreferrer">
            View project on GitHub
          </a>
          <a class="pill ghost" href="../index.html#projects">
            Back to Project Gallery
          </a>
        </div>
      </div>
      <div style="flex:1"></div>
    </div>
  </header>

  <main class="container">
    <!-- Synopsis -->
    <section class="synopsis" id="synopsis">
      <h2>Synopsis</h2>
      <p>
        Sign language users communicate through hand shapes and movements, but most digital systems cannot understand these gestures. This research builds a system that automatically translates American Sign Language fingerspelling, where each hand shape represents a letter, into written English text. The system first tracks the 3D positions of hands and upper body joints in video using existing computer vision software. These position measurements are then fed into a neural network that understands sequences and context, learning to recognize individual letters and detect when words end. When tested on new videos, the system achieved a character error rate of approximately 0.36, meaning it correctly identified about 64% of characters. This represents a meaningful step toward making sign language more accessible through automated translation.
      </p>
      <div class="chips" aria-label="Key tags">
        <span class="chip">MediaPipe landmarks</span>
        <span class="chip">Transformer encoder and decoder</span>
        <span class="chip">CER ≈ 0.36 (validation)</span>
        <span class="chip">Sequence normalization</span>
        <span class="chip">Greedy decoding</span>
      </div>
    </section>

    <div class="grid" style="margin-top:18px">
      <!-- TOC -->
      <nav class="toc" aria-label="Table of contents">
        <h3>Contents</h3>
        <a href="#problem">1. Problem and users</a>
        <a href="#solution">2. Solution overview</a>
        <a href="#data">3. Data and feature spec</a>
        <a href="#normalization">4. Normalization and serialization</a>
        <a href="#architecture">5. Model architecture</a>
        <a href="#training">6. Training setup</a>
        <a href="#metric">7. Metric</a>
        <a href="#results">8. Results</a>
        <a href="#limits">9. Limits and next steps</a>
        <a href="#faq">FAQ</a>
      </nav>

      <!-- Content -->
      <div class="content">
        <section id="problem">
          <h2>1. Problem and users</h2>
          <p>
            Fingerspelling conveys names and terms without dedicated signs. Recognition is difficult due to fast transitions and co articulation between letters. The goal is letter accurate transcription with clear handling of similar handshapes and consistent preprocessing across signers and cameras.
          </p>
        </section>

        <section id="solution">
          <h2>2. Solution overview</h2>
          <p>
            The system replaces raw pixels with landmarks and frames the task as sequence to sequence prediction. MediaPipe provides 3D keypoints per frame. A Transformer captures temporal context and outputs letters A to Z plus an end token. The pipeline includes careful feature selection and normalization for stable learning.
          </p>
          <div class="chips">
            <span class="chip">Sequence to sequence</span>
            <span class="chip">On device landmarks</span>
            <span class="chip">Context aware decoding</span>
          </div>
        </section>

        <section id="data">
          <h2>3. Data and feature spec</h2>
          <h3>3.1 Dataset</h3>
          <p>
            The build uses the Google American Sign Language Fingerspelling dataset with precomputed MediaPipe landmarks. Examples are landmark sequences paired with target words.
          </p>
          <h3>3.2 Feature selection (F = 144 per frame)</h3>
          <ul>
            <li>Pose joints: six upper body joints 11, 12, 13, 14, 15, 16.</li>
            <li>Left hand: 21 landmarks 0 to 20.</li>
            <li>Right hand: 21 landmarks 0 to 20.</li>
            <li>Ordering per frame: pose, then left hand, then right hand. Each landmark contributes x, y, z so 48 triplets times 3 equals 144 numbers.</li>
          </ul>
        </section>

        <section id="normalization">
          <h2>4. Normalization and serialization</h2>
          <ul>
            <li><strong>Center</strong>: mid shoulders if pose present, else mid wrists.</li>
            <li><strong>Scale</strong>: shoulder distance if pose present, else wrist distance.</li>
            <li><strong>Sanitize</strong>: fill short gaps by linear then forward and backward fill. Replace NaN and Inf with zeros. Clamp to a safe numeric range. Cast to float32.</li>
            <li><strong>Axes</strong>: follow MediaPipe coordinate conventions.</li>
            <li><strong>Storage</strong>: fast binary arrays with sequence masks so attention ignores padding.</li>
          </ul>
          <p class="small">
            These steps are mirrored wherever the model is evaluated so distribution shift is minimized.
          </p>
        </section>

        <section id="architecture">
          <h2>5. Model architecture</h2>
          <h3>5.1 Encoder and decoder</h3>
          <p>
            The encoder ingests the landmark sequence and builds contextual representations with multi head self attention and position wise feed forward layers. The decoder generates characters step by step using masked self attention and cross attention over encoder outputs. Positional signals are added on both sides so order is represented.
          </p>
          <h3>5.2 Practical size</h3>
          <p>
            A moderate footprint balances latency and accuracy. Typical settings are around four to six layers on both encoder and decoder, about eight heads, and model width near 256 to 512. Dropout after attention and after feed forward helps regularize. Residual connections and layer normalization follow the standard Transformer design.
          </p>
          <h3>5.3 Output vocabulary</h3>
          <p>
            Characters A to Z plus an end token. A linear layer maps to the character set followed by softmax. Greedy decoding is sufficient for spelling.
          </p>
          <table aria-label="Model summary">
            <thead><tr><th>Component</th><th>Choice</th><th>Notes</th></tr></thead>
            <tbody>
              <tr><td>Input</td><td>F = 144 features per frame</td><td>Pose 11 to 16, both hands 0 to 20</td></tr>
              <tr><td>Encoder</td><td>Self attention</td><td>Sinusoidal positional signals</td></tr>
              <tr><td>Decoder</td><td>Masked self attention plus cross attention</td><td>Greedy decode</td></tr>
              <tr><td>Width</td><td>≈ 256 to 512</td><td>With dropout, residual, layer norm</td></tr>
              <tr><td>Output</td><td>Letters A to Z plus EOS</td><td>Softmax over characters</td></tr>
            </tbody>
          </table>
        </section>

        <section id="training">
          <h2>6. Training setup</h2>
          <ul>
            <li>Loss: sequence cross entropy with teacher forcing.</li>
            <li>Optimization: Adam with warmup then decay.</li>
            <li>Batching: bucket by length, pad, and mask attention on padded positions.</li>
            <li>Validation: track character error rate and select the checkpoint with lowest validation CER.</li>
          </ul>
        </section>

        <section id="metric">
          <h2>7. Metric</h2>
          <p>
            <strong>Character Error Rate</strong> measures character level edits relative to reference length. This is the primary metric and the basis for model selection.
          </p>
        </section>

        <section id="results">
          <h2>8. Results</h2>
          <table aria-label="Results at a glance">
            <thead><tr><th>Metric</th><th>Value</th><th>Notes</th></tr></thead>
            <tbody>
              <tr>
                <td>CER on validation</td>
                <td>≈ 0.36</td>
                <td>From the project repository</td>
              </tr>
            </tbody>
          </table>
          <div class="callout">
            Common confusions include similar shapes like I vs J, M vs N, and U vs V. Double letters may need a brief pause for clear separation.
          </div>
        </section>

        <section id="limits">
          <h2>9. Limits and next steps</h2>
          <ul>
            <li>Scope is fingerspelling letters, not the full sign lexicon.</li>
            <li>Very fast signing can cause dropped or merged letters.</li>
            <li>Lighting, occlusion, or partial hands can degrade landmarks.</li>
          </ul>
          <p class="small">
            Next steps include light personalization with a few user examples, optional beam search for long words, and mobile deployment with quantization.
          </p>
        </section>

        <section id="faq">
          <h2>FAQ</h2>
          <h3>Why landmarks instead of pixels</h3>
          <p>
            Landmarks expose the essential signal which is hand motion. They reduce input size and speed up training while improving robustness across camera setups.
          </p>
          <h3>Why a Transformer for letters</h3>
          <p>
            It models temporal context so the system can disambiguate similar handshapes by using motion before and after each frame.
          </p>
          <h3>How is the final checkpoint chosen</h3>
          <p>
            By the lowest validation character error rate on a held out split.
          </p>
        </section>
      </div>
    </div>

    
  </main>
</body>
</html>
