<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Transformer-Based ASL Fingerspelling to Text</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="A lucid, in-depth page for a Transformer-based ASL fingerspelling-to-text product. Includes data pipeline, model, training, metrics (CER), smoothing, and a real-time demo loop." />
  <style>
    :root{
      --bg:#fbf8f2;
      --card:#ffffff;
      --ink:#0f172a;
      --muted:#64748b;
      --line:#e6e8ec;
      --chip:#f6f7fb;

      --accent:#2140ff;
      --accent-hover:#1735d1;
      --glow:rgba(33,64,255,.18);
    }

    *{box-sizing:border-box}
    html,body{height:100%}
    body{
      margin:0; background:var(--bg); color:var(--ink);
      font:16px/1.6 system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,"Helvetica Neue",Arial;
      -webkit-font-smoothing:antialiased; -moz-osx-font-smoothing:grayscale;
    }
    a{color:var(--accent); text-decoration:none}
    a:hover{opacity:.95}

    .container{max-width:1100px; margin:0 auto; padding:24px}

    header{
      position:sticky; top:0; z-index:50;
      backdrop-filter:saturate(1.05) blur(8px);
      background:rgba(251,248,242,.9);
      border-bottom:1px solid var(--line);
    }
    .header-inner{display:flex; align-items:flex-start; gap:16px; padding:14px 24px; max-width:1100px; margin:0 auto; flex-wrap:wrap}
    .brand{display:flex; flex-direction:column; gap:8px}
    .brand h1{margin:0; font-size:20px}
    .actions{display:flex; gap:12px; flex-wrap:wrap}

    .pill{
      display:inline-flex; align-items:center; gap:10px;
      padding:10px 16px; border-radius:999px; font-weight:600; border:1px solid transparent;
      transition:transform .06s ease, background-color .2s ease, border-color .2s ease, color .2s ease, box-shadow .2s ease;
      cursor:pointer;
    }
    .pill.primary{background:var(--accent); color:#fff; box-shadow:0 10px 24px var(--glow)}
    .pill.primary:hover{background:var(--accent-hover); transform:translateY(-1px)}
    .pill.ghost{background:#fff; color:var(--ink); border:1px solid var(--line)}
    .pill.ghost:hover{transform:translateY(-1px); background:#fafcff}

    main{padding:24px}
    section{background:var(--card); border:1px solid var(--line); border-radius:16px; padding:22px}
    .synopsis h2{margin:0 0 10px}
    h2{margin:0 0 12px; font-size:24px}
    h3{margin:16px 0 8px; font-size:18px}
    p{margin:10px 0}

    .grid{display:grid; gap:18px}
    @media(min-width:980px){ .grid{grid-template-columns:280px 1fr} }

    nav.toc{
      position:sticky; top:96px; align-self:start; background:var(--card); border:1px solid var(--line); border-radius:14px; padding:14px;
    }
    nav.toc h3{margin:6px 6px 10px; font-size:13px; color:var(--muted); letter-spacing:.4px; text-transform:uppercase}
    nav.toc a{
      display:block; color:var(--ink); padding:8px 10px; border-radius:10px; font-size:14px; border:1px solid transparent;
    }
    nav.toc a:hover{background:#f7f8ff; border-color:var(--line)}

    .chips{display:flex; flex-wrap:wrap; gap:8px; margin-top:8px}
    .chip{background:var(--chip); border:1px solid var(--line); padding:6px 10px; border-radius:999px; font-size:13px; color:var(--muted)}

    .callout{padding:12px 14px; border-left:4px solid var(--accent); background:#f3f6ff; border-radius:10px}

    table{width:100%; border-collapse:separate; border-spacing:0; overflow:hidden; border-radius:12px; margin:12px 0}
    thead th{background:#f6f7fb; color:var(--muted); font-weight:600; text-align:left; padding:10px; border-bottom:1px solid var(--line)}
    tbody td{padding:10px; border-bottom:1px solid var(--line)}
    tbody tr:hover td{background:#fafcff}

    footer{color:var(--muted); text-align:center; padding:36px 0}
    .small{font-size:13px; color:var(--muted)}
  </style>
</head>
<body>
  <header>
    <div class="header-inner">
      <div class="brand">
        <h1>Transformer-Based ASL Fingerspelling to Text</h1>
        <div class="actions">
          <!-- Keep this file name exactly as your attached PDF -->
          <a class="pill primary" href="Transformer-Based%20ASL%20Fingerspelling-to-Text%20System.pdf" download>
            ‚¨áÔ∏è Download the full report <small>(PDF)</small>
          </a>
          <a class="pill ghost" href="https://github.com/Shabbir-Tashrifwala/ASL-Fingerspelling-to-Text" target="_blank" rel="noreferrer">
            üîó View project on GitHub
          </a>
        </div>
      </div>
      <div style="flex:1"></div>
    </div>
  </header>

  <main class="container">
    <!-- Synopsis -->
    <section class="synopsis" id="synopsis">
      <h2>Synopsis</h2>
      <p>
        This product translates live ASL fingerspelling into readable English text. It uses MediaPipe to extract 3D landmarks for both hands and key upper body joints, converts each frame to a fixed length numeric vector, and feeds the sequence into a compact Transformer. The model predicts letters with context and an end token to finish words. A real time loop with a sliding window and temporal smoothing makes on screen text stable. On validation, the system attains a character error rate around 0.36. Word error is higher by nature, but most words are fully correct in typical tests.
      </p>
      <div class="chips" aria-label="Key tags">
        <span class="chip">MediaPipe landmarks</span>
        <span class="chip">Transformer encoder and decoder</span>
        <span class="chip">CER ‚âà 0.36 (validation)</span>
        <span class="chip">Sliding window</span>
        <span class="chip">Temporal smoothing</span>
        <span class="chip">Real time CLI demo</span>
      </div>
    </section>

    <div class="grid" style="margin-top:18px">
      <!-- TOC -->
      <nav class="toc" aria-label="Table of contents">
        <h3>Contents</h3>
        <a href="#problem">1. Problem and users</a>
        <a href="#solution">2. Solution overview</a>
        <a href="#data">3. Data and feature spec</a>
        <a href="#normalization">4. Normalization and serialization</a>
        <a href="#architecture">5. Model architecture</a>
        <a href="#training">6. Training setup</a>
        <a href="#metrics">7. Metrics</a>
        <a href="#results">8. Results</a>
        <a href="#realtime">9. Real time inference</a>
        <a href="#limits">10. Limits and next steps</a>
        <a href="#repo">11. Repo layout and quick start</a>
        <a href="#faq">FAQ</a>
      </nav>

      <!-- Content -->
      <div class="content">
        <section id="problem">
          <h2>1. Problem and users</h2>
          <p>
            Fingerspelling communicates names and terms that lack a dedicated sign. Recognizing it from video is challenging due to fast transitions and co articulation between letters. A practical tool needs low latency, low flicker, and strong letter level accuracy so the text reads naturally.
          </p>
        </section>

        <section id="solution">
          <h2>2. Solution overview</h2>
          <p>
            The system replaces raw pixels with structured landmarks, then treats recognition as sequence to sequence prediction. MediaPipe yields 3D keypoints per frame. A Transformer models temporal context and outputs letters A to Z plus an end token. The app layer runs capture, preprocessing, streaming inference, and stabilized text rendering.
          </p>
          <div class="chips">
            <span class="chip">Sequence to sequence</span>
            <span class="chip">Greedy decoding</span>
            <span class="chip">On device landmarks</span>
          </div>
        </section>

        <section id="data">
          <h2>3. Data and feature spec</h2>
          <h3>3.1 Dataset</h3>
          <p>
            The build uses the Google American Sign Language Fingerspelling dataset with precomputed MediaPipe landmarks. Training examples are landmark sequences paired with target words.
          </p>
          <h3>3.2 Feature selection (F = 144 per frame)</h3>
          <ul>
            <li>Pose joints: 6 upper body joints [11, 12, 13, 14, 15, 16].</li>
            <li>Left hand: 21 landmarks (0 to 20).</li>
            <li>Right hand: 21 landmarks (0 to 20).</li>
            <li>Order per frame: pose, then left hand, then right hand; each as (x, y, z) so 48 triplets √ó 3 = 144 numbers.</li>
          </ul>
        </section>

        <section id="normalization">
          <h2>4. Normalization and serialization</h2>
          <ul>
            <li><strong>Center</strong>: mid shoulders if pose present, else mid wrists.</li>
            <li><strong>Scale</strong>: shoulder distance if pose present, else wrist distance.</li>
            <li><strong>Sanitize</strong>: fill small gaps by linear then forward and backward fill; replace NaN and Inf with zeros; clamp to a safe range; cast to float32.</li>
            <li><strong>Consistent axes</strong>: follow MediaPipe coordinate conventions.</li>
            <li><strong>Storage</strong>: fast binary arrays with sequence masks for padding during training.</li>
          </ul>
        </section>

        <section id="architecture">
          <h2>5. Model architecture</h2>
          <h3>5.1 Encoder and decoder</h3>
          <p>
            The encoder reads landmark sequences and builds contextual representations with multi head self attention and position wise feed forward layers. The decoder predicts characters step by step with masked self attention and cross attention over encoder outputs. Positional signals are added on both sides.
          </p>
          <h3>5.2 Practical size</h3>
          <p>
            A moderate footprint balances latency and accuracy. Typical settings are about 4 to 6 layers on each side, around 8 heads, and model width in the low hundreds such as 256 or 512. Dropout after attention and after feed forward helps regularize. Residual connections and layer normalization follow the standard Transformer design.
          </p>
          <h3>5.3 Output vocabulary</h3>
          <p>
            Characters A to Z plus an end token. A linear layer maps to the character set followed by softmax. Greedy decoding works well for spelling and real time use.
          </p>
          <table aria-label="Model summary">
            <thead><tr><th>Component</th><th>Choice</th><th>Notes</th></tr></thead>
            <tbody>
              <tr><td>Input</td><td>F = 144 features per frame</td><td>Pose 11 to 16, both hands 0 to 20</td></tr>
              <tr><td>Encoder</td><td>Self attention</td><td>Sinusoidal positional signals</td></tr>
              <tr><td>Decoder</td><td>Masked self attention + cross attention</td><td>Greedy decode</td></tr>
              <tr><td>Width</td><td>‚âà 256 to 512</td><td>With dropout, residual, layer norm</td></tr>
              <tr><td>Output</td><td>Letters A to Z + EOS</td><td>Softmax over characters</td></tr>
            </tbody>
          </table>
        </section>

        <section id="training">
          <h2>6. Training setup</h2>
          <ul>
            <li>Loss: sequence cross entropy with teacher forcing.</li>
            <li>Optimization: Adam with warmup then decay.</li>
            <li>Batching: bucket by length, pad and mask attention on padded positions.</li>
            <li>Validation: split from training data; track CER and WER curves to pick the best checkpoint by lowest validation CER.</li>
          </ul>
        </section>

        <section id="metrics">
          <h2>7. Metrics</h2>
          <p>
            The system reports character error rate and word error rate. CER measures letter level edits relative to reference length. WER measures word level errors and is stricter since any letter error can mark a word wrong.
          </p>
        </section>

        <section id="results">
          <h2>8. Results</h2>
          <table aria-label="Results at a glance">
            <thead><tr><th>Metric</th><th>Value</th><th>Notes</th></tr></thead>
            <tbody>
              <tr>
                <td>CER (validation)</td>
                <td>‚âà 0.36</td>
                <td>From the project repository</td>
              </tr>
              <tr>
                <td>WER</td>
                <td>Qualitatively higher than CER</td>
                <td>Majority of words fully correct on held out tests</td>
              </tr>
            </tbody>
          </table>
          <div class="callout">
            Common confusions include similar shapes like I vs J, M vs N, and U vs V. Double letters may need a brief pause for clear separation.
          </div>
        </section>

        <section id="realtime">
          <h2>9. Real time inference</h2>
          <h3>9.1 Streaming loop</h3>
          <ul>
            <li>Capture webcam frames.</li>
            <li>Extract landmarks and apply the same normalization as training.</li>
            <li>Maintain a rolling window of recent frames, for example around 30 frames which is roughly 3 seconds at 10 fps.</li>
            <li>Run the encoder on the window and decode letters until the end token or a safe limit.</li>
          </ul>
          <h3>9.2 Temporal smoothing</h3>
          <p>
            A moving average over next token probabilities plus a short stability gate reduces flicker. The UI commits a letter only after consistent confidence. When the end token is stable the word is finalized and the buffer resets.
          </p>
          <h3>9.3 Performance notes</h3>
          <p>
            MediaPipe typically processes a frame in under about 20 milliseconds on a laptop. The overall experience is near real time with a small delay due to windowing and smoothing.
          </p>
        </section>

        <section id="limits">
          <h2>10. Limits and next steps</h2>
          <ul>
            <li>Scope is fingerspelling letters, not the full sign lexicon yet.</li>
            <li>Very fast signing can cause dropped or merged letters.</li>
            <li>Lighting, occlusion, or partial hands can degrade landmarks.</li>
          </ul>
          <p class="small">
            Next steps: light personalization with a few user examples, optional beam search for long words, and mobile deployment with quantization.
          </p>
        </section>

        <section id="repo">
          <h2>11. Repo layout and quick start</h2>
          <h3>Structure</h3>
          <pre class="small" aria-label="Repo tree">
assets/           # weights and character maps
notebooks/        # exploratory notebook
src/aslfs_rt/     # library package for app + model
  app_cli.py      # webcam demo
  config.py       # small config helpers
  model.py        # Transformer
  preprocess.py   # feature extraction and normalization
README.md         # usage guide
          </pre>
          <h3>Requirements</h3>
          <pre class="small">
numpy
opencv-python
mediapipe==0.10.14
torch
          </pre>
          <h3>Run the demo</h3>
          <pre class="small">
# Create and activate a venv, then:
pip install -r requirements.txt

# Put your trained weights at:
assets/model_phase2_best.pt

# Start the webcam app:
python -m aslfs_rt.app_cli --bundle . --source 0 --flip
          </pre>
        </section>

        <section id="faq">
          <h2>FAQ</h2>
          <h3>Why landmarks instead of pixels</h3>
          <p>
            Landmarks expose the true signal which is hand motion. They cut input size and speed up training and inference while improving robustness across cameras.
          </p>
          <h3>Why a Transformer for letters</h3>
          <p>
            It models context over time so the system can disambiguate similar handshapes by looking at motion before and after each frame.
          </p>
          <h3>How do you pick the final checkpoint</h3>
          <p>
            By the lowest validation CER with stable WER, confirmed on a held out split.
          </p>
        </section>
      </div>
    </div>

    <footer>
      <p class="small">
        Place this HTML file in the same folder as <em>Transformer-Based ASL Fingerspelling-to-Text System.pdf</em> to enable the download button.
      </p>
    </footer>
  </main>
</body>
</html>
